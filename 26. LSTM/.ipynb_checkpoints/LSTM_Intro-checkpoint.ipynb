{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing or Exploiting Gradients\n",
    "\n",
    "Effect of vanishing gradient problem with respect to initial input is:\n",
    "    * Variations in initial input shows negligible effect on final output \n",
    "    * Weights for initial inputs change negligibly \n",
    "\n",
    "Effect of exploding gradient problem with respect to initial input is :\n",
    "    * Variations in initial input shows huge effect on final output \n",
    "    * Weights for initial inputs change exponentially \n",
    "\n",
    "Vanishing gradient problem is faced when output is generated by passing input to a large number of hidden layers, this problem occurs in RNN where a single layer of RNN units is used because:\n",
    "    * Output of RNN layer is decided by back propagating on all its initial values resulting in multiple weight updates. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units\n",
    "\n",
    "The purpose of using GRU is:\n",
    "\n",
    "    * Solve vanishing gradient problem in RNN \n",
    "    * Enable memory unit to decide whether to update its memory output for current input or not \n",
    "    * Get a way to pass a_i-1 as a_i without making any changes based on current input \n",
    "\n",
    "GRU units are faster to train as:\n",
    "    * They tend work better on smaller datasets \n",
    "    * They involve less weights to train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations of GRU\n",
    "\n",
    "The two gates in a GRU are: \n",
    "* reset : Work to determine how much of the past information is to be forgotten.\n",
    "* Update : Work to determine how much of the past information is needed to be passed along future.\n",
    "\n",
    "Minimal gated unit uses equal weights and biases for reset and update gate ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory - LSTM\n",
    "\n",
    "The memory cell has no activation applied on to give C_t.\n",
    "\n",
    "The purpose of not applying activation on memory cell is to : Prevent the vanishing gradient as without activation weights are not changed drastically when back propagated through time.\n",
    "\n",
    "The activation function usually used in the input gate of a LSTM is: **Sigmoid**\n",
    "\n",
    "Purpose of output gate : To decide extent to which C_t affect activation output.\n",
    "\n",
    "In the equation to calculate C_t the function of input(i) and forget(f) gates respectively :\n",
    "    * Input gate decides the contribution of new candidate for memory in C_t\n",
    "    * Forget gate decides contribution of previous memory cell value in C_t \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
