{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport os\nimport time", "outputs": [], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "start_time = time.time()\nstart_time", "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "1525528202.7059984"}, "metadata": {}}], "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "import nltk\nnltk.download('stopwords')", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}, {"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "True"}, "metadata": {}}], "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nstop_words", "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "{'a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been',\n 'before',\n 'being',\n 'below',\n 'between',\n 'both',\n 'but',\n 'by',\n 'can',\n 'couldn',\n \"couldn't\",\n 'd',\n 'did',\n 'didn',\n \"didn't\",\n 'do',\n 'does',\n 'doesn',\n \"doesn't\",\n 'doing',\n 'don',\n \"don't\",\n 'down',\n 'during',\n 'each',\n 'few',\n 'for',\n 'from',\n 'further',\n 'had',\n 'hadn',\n \"hadn't\",\n 'has',\n 'hasn',\n \"hasn't\",\n 'have',\n 'haven',\n \"haven't\",\n 'having',\n 'he',\n 'her',\n 'here',\n 'hers',\n 'herself',\n 'him',\n 'himself',\n 'his',\n 'how',\n 'i',\n 'if',\n 'in',\n 'into',\n 'is',\n 'isn',\n \"isn't\",\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'just',\n 'll',\n 'm',\n 'ma',\n 'me',\n 'mightn',\n \"mightn't\",\n 'more',\n 'most',\n 'mustn',\n \"mustn't\",\n 'my',\n 'myself',\n 'needn',\n \"needn't\",\n 'no',\n 'nor',\n 'not',\n 'now',\n 'o',\n 'of',\n 'off',\n 'on',\n 'once',\n 'only',\n 'or',\n 'other',\n 'our',\n 'ours',\n 'ourselves',\n 'out',\n 'over',\n 'own',\n 're',\n 's',\n 'same',\n 'shan',\n \"shan't\",\n 'she',\n \"she's\",\n 'should',\n \"should've\",\n 'shouldn',\n \"shouldn't\",\n 'so',\n 'some',\n 'such',\n 't',\n 'than',\n 'that',\n \"that'll\",\n 'the',\n 'their',\n 'theirs',\n 'them',\n 'themselves',\n 'then',\n 'there',\n 'these',\n 'they',\n 'this',\n 'those',\n 'through',\n 'to',\n 'too',\n 'under',\n 'until',\n 'up',\n 've',\n 'very',\n 'was',\n 'wasn',\n \"wasn't\",\n 'we',\n 'were',\n 'weren',\n \"weren't\",\n 'what',\n 'when',\n 'where',\n 'which',\n 'while',\n 'who',\n 'whom',\n 'why',\n 'will',\n 'with',\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\",\n 'y',\n 'you',\n \"you'd\",\n \"you'll\",\n \"you're\",\n \"you've\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves'}"}, "metadata": {}}], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "# Our own list of some block words to be avoided  \nblock_words = ['newsgroups', 'xref', 'path', 'from', 'subject', 'sender', 'organisation', 'apr','gmt', 'last','better','never','every','even','two','good','used','first','need','going','must','really','might','well','without','made','give','look','try','far','less','seem','new','make','many','way','since','using','take','help','thanks','send','free','may','see','much','want','find','would','one','like','get','use','also','could','say','us','go','please','said','set','got','sure','come','lot','seems','able','anything','put', '--', '|>', '>>', '93', 'xref', 'cantaloupe.srv.cs.cmu.edu', '20', '16', \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\", '21', '19', '10', '17', '24', 'reply-to:', 'thu', 'nntp-posting-host:', 're:','25''18'\"i'd\"'>i''22''fri,''23''>the','references:','xref:','sender:','writes:','1993','organization:']\n", "outputs": [], "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "import urllib.request\nurllib.request.urlretrieve (\"https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\", \"a.tar.gz\")", "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "('a.tar.gz', <http.client.HTTPMessage at 0x7f345acd8438>)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "import tarfile\ntar = tarfile.open(\"a.tar.gz\")\ntar.extractall()\ntar.close()", "outputs": [], "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "##Make a list of the folders in the dataset\ndirectory = [f for f in os.listdir('./20_newsgroups') if not f.startswith('.')]\ndirectory", "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "['comp.windows.x',\n 'comp.sys.ibm.pc.hardware',\n 'comp.graphics',\n 'rec.motorcycles',\n 'misc.forsale',\n 'sci.space',\n 'sci.electronics',\n 'talk.politics.misc',\n 'soc.religion.christian',\n 'talk.politics.mideast',\n 'rec.sport.baseball',\n 'talk.politics.guns',\n 'sci.med',\n 'talk.religion.misc',\n 'alt.atheism',\n 'rec.sport.hockey',\n 'comp.os.ms-windows.misc',\n 'rec.autos',\n 'comp.sys.mac.hardware',\n 'sci.crypt']"}, "metadata": {}}], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "vocab = {}\nfor i in range(len(directory)):\n    ##Create a list of files in the given dictionary \n    files = os.listdir('./20_newsgroups/' + directory[i])\n \n    for j in range(len(files)):\n        ##Path of each file \n        path = './20_newsgroups/' + directory[i] + '/' + files[j]\n        \n        ##open the file and read it\n        text = open(path, 'r', errors='ignore').read()\n        \n        for word in text.split():\n            \n            ## If word doesnt contain any special character then create the dictionary\n            if len(word) != 1:  \n                \n                ##Check if word is a non stop word or non block word(we have created) only then proceed\n                if not word.lower() in stop_words:\n                    if not word.lower() in block_words:     \n                        ##If word is already in dictionary then we just increment its frequency by 1\n                        if vocab.get(word.lower()) != None:\n                            vocab[word.lower()] += 1\n\n                        ##If word is not in dictionary then we put that word in our dictinary by making its frequnecy 1\n                        else:\n                            vocab[word.lower()] = 1\n            \n# vocab", "outputs": [], "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "import operator\nsorted_vocab = sorted(vocab.items(), key= operator.itemgetter(1), reverse= True)\n# sorted_vocab", "outputs": [], "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "# Dictionary containing the most occuring k-words.\nkvocab={}\n\n# Frequency of 1000th most occured word\nz = sorted_vocab[2000][1]\n\nfor x in sorted_vocab:\n    kvocab[x[0]] = x[1]\n    \n    if x[1] <= z:\n        break", "outputs": [], "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "len(kvocab)", "outputs": [{"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "2001"}, "metadata": {}}], "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "sorted_vocab[0:100]", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "[('subject:', 20486),\n ('from:', 20417),\n ('date:', 20137),\n ('newsgroups:', 20081),\n ('message-id:', 20050),\n ('lines:', 20042),\n ('path:', 20029),\n ('article', 12108),\n ('people', 8415),\n ('university', 8203),\n ('know', 7695),\n ('think', 7205),\n (\"i'm\", 5823),\n ('distribution:', 4406),\n ('time', 4336),\n ('it.', 4185),\n ('anyone', 3976),\n ('world', 3602),\n ('right', 3326),\n ('believe', 3309),\n ('still', 3290),\n ('something', 3190),\n ('computer', 3157),\n ('system', 3137),\n (\"i've\", 3114),\n ('15', 2881),\n ('god', 2881),\n ('back', 2840),\n (\"can't\", 2836),\n ('news', 2836),\n ('state', 2787),\n ('work', 2692),\n ('>in', 2610),\n ('someone', 2610),\n ('government', 2534),\n ('problem', 2528),\n ('23', 2522),\n ('another', 2516),\n ('read', 2516),\n ('usa', 2496),\n ('information', 2480),\n ('>the', 2452),\n ('number', 2424),\n (\"that's\", 2382),\n ('things', 2378),\n ('part', 2323),\n ('fri,', 2307),\n ('point', 2297),\n ('little', 2294),\n ('22', 2284),\n ('windows', 2265),\n ('>i', 2253),\n ('tue,', 2241),\n ('file', 2208),\n ('data', 2155),\n ('question', 2126),\n ('probably', 2112),\n ('years', 2106),\n ('different', 2100),\n ('available', 2095),\n ('(usenet', 2079),\n ('space', 2079),\n ('it,', 2073),\n ('around', 2072),\n ('long', 2053),\n ('tell', 2048),\n ('least', 2006),\n ('best', 1997),\n ('program', 1995),\n ('software', 1976),\n ('public', 1961),\n ('power', 1958),\n ('thu,', 1883),\n ('thing', 1875),\n ('drive', 1870),\n ('run', 1869),\n ('support', 1864),\n ('however,', 1826),\n (\"i'd\", 1825),\n ('18', 1804),\n ('rather', 1801),\n ('enough', 1792),\n ('case', 1791),\n ('hard', 1786),\n ('keep', 1770),\n ('fact', 1767),\n ('25', 1758),\n ('let', 1757),\n ('science', 1753),\n ('called', 1751),\n ('great', 1742),\n ('...', 1738),\n ('call', 1725),\n ('looking', 1709),\n ('mon,', 1690),\n ('found', 1683),\n ('real', 1676),\n ('nothing', 1671),\n ('26', 1661),\n ('quite', 1634)]"}, "metadata": {}}], "metadata": {"scrolled": true}}, {"execution_count": 14, "cell_type": "code", "source": "features_list = list(kvocab.keys())\n\n## Create a Dataframe containing features_list as columns \ndf = pd.DataFrame(columns = features_list)\n\n\n## Filling the x_train values in dataframe \n\nfor i in range(len(directory)):\n    ##Create a list of files in the given dictionary \n    files = os.listdir('./20_newsgroups/' + directory[i])\n \n    for j in range(len(files)):\n        ##Insert a row at the end of Dataframe with all zeros\n        df.loc[len(df)] = np.zeros(len(features_list))\n        \n        ##Path of each file \n        path = './20_newsgroups/' + directory[i] + '/' + files[j]\n        \n        ##open the file and read it\n        text = open(path, 'r', errors='ignore').read()\n        \n        \n        for word in text.split():\n            if word.lower() in features_list:\n                df[word.lower()][len(df)-1] += 1\n                \n\n# df.head()", "outputs": [], "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "# df.describe()", "outputs": [], "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "## Making the 2d array of x\nx = df.values\nx.shape", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "(19997, 2001)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "## Creating  y array containing labels for classification \n\ny = []\n\nfor i in range(len(directory)):\n    ##Create a list of files in the given dictionary \n    files = os.listdir('./20_newsgroups/' + directory[i])\n \n    for j in range(len(files)):\n        y.append(i)\n\ny = np.array(y)\ny.shape", "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "(19997,)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "from sklearn import model_selection\nx_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.25, random_state = 0)", "outputs": [], "metadata": {}}, {"source": "## Implement Multinomial Naive Bayes from sklearn", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\n\ntrain_score = clf.score(x_train, y_train)\ntest_score = clf.score(x_test, y_test)\n\ntrain_score, test_score", "outputs": [{"execution_count": 19, "output_type": "execute_result", "data": {"text/plain": "(0.8784423551376942, 0.83420000000000005)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "end_time = time.time()\ntotal_time = end_time - start_time\ntotal_time", "outputs": [{"execution_count": 20, "output_type": "execute_result", "data": {"text/plain": "3898.0185787677765"}, "metadata": {}}], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "widgets": {"application/vnd.jupyter.widget-state+json": {"state": {}, "version_minor": 0, "version_major": 2}}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.4.5", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}