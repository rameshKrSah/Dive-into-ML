{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Movie Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2000 reviews out of which 1000 are positive and 1000 negative\n",
    "len(movie_reviews.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_reviews.fileids(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_reviews.fileids(\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['capsule', ':', 'in', '2176', 'on', 'the', 'planet', ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words(movie_reviews.fileids(\"neg\")[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg'),\n",
       " (['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...], 'neg'),\n",
       " (['it', 'is', 'movies', 'like', 'these', 'that', 'make', ...], 'neg'),\n",
       " (['\"', 'quest', 'for', 'camelot', '\"', 'is', 'warner', ...], 'neg'),\n",
       " (['synopsis', ':', 'a', 'mentally', 'unstable', 'man', ...], 'neg')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a documents list in which each item has all the tokenised words from a text document and their corresponding categories\n",
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids():\n",
    "        documents.append((movie_reviews.words(fileid), category))\n",
    "        \n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "## Function to change the pos_tag to simpler values which can be passed to lemmatize\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('better', 'RBR')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"better\"\n",
    "pos_tag([w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to clean the words by removing stopwords, taking care of cases and lemmatizing the word to give root words only\n",
    "\n",
    "def clean_review(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop:\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [(clean_review(words), category) for words, category in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling and Splitting Training and Testing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Shuffling the document since earlier first 1000 where negative and next 1000 where of positive category.\n",
    "import random\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_documents = documents[0:1500]\n",
    "testing_documents = documents[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Feature Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK classifier requires data in format:\n",
    "[ ( { f1: #val, f2: #val }, category ), \n",
    "  (...), (...), ...]\n",
    "  \n",
    "A list of tuples for each document where each tuple contains a dictionary of features with corresponding values and the category of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for doc in training_documents:\n",
    "    all_words += doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frequency distribution of each words\n",
    "freq = nltk.FreqDist(all_words)\n",
    "# freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 8239),\n",
       " ('movie', 5311),\n",
       " ('one', 4518),\n",
       " ('make', 3251),\n",
       " ('character', 2949),\n",
       " ('like', 2832),\n",
       " ('get', 2719),\n",
       " ('go', 2312),\n",
       " ('see', 2253),\n",
       " ('time', 2205),\n",
       " ('well', 2158),\n",
       " ('scene', 2013),\n",
       " ('even', 1859),\n",
       " ('good', 1830),\n",
       " ('story', 1771)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Top 3000 common words\n",
    "common = freq.most_common(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features list\n",
    "features = [i[0] for i in common]\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to create the dictionary for each document with boolean values for each features\n",
    "def get_features_dict(words):\n",
    "    ## Creating a feaure dictionary for each document \n",
    "    current_features = {}\n",
    "    words_set = set(words)\n",
    "    \n",
    "    for w in features:\n",
    "        current_features[w] = w in words_set\n",
    "    \n",
    "    return current_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_features_dict(training_documents[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = [(get_features_dict(doc), category) for doc, category in training_documents]\n",
    "testing_data = [(get_features_dict(doc), category) for doc, category in testing_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using NLTK Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.332"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 quentin = True              neg : pos    =      3.7 : 1.0\n",
      "                    weir = True              neg : pos    =      3.6 : 1.0\n",
      "                   reese = True              pos : neg    =      3.5 : 1.0\n",
      "                 branagh = True              pos : neg    =      3.2 : 1.0\n",
      "                    dora = True              neg : pos    =      3.1 : 1.0\n",
      "                    echo = True              neg : pos    =      3.1 : 1.0\n",
      "                 gorilla = True              pos : neg    =      2.9 : 1.0\n",
      "                  ripley = True              pos : neg    =      2.9 : 1.0\n",
      "                    todd = True              neg : pos    =      2.7 : 1.0\n",
      "                  tarzan = True              pos : neg    =      2.7 : 1.0\n",
      "                 amistad = True              pos : neg    =      2.5 : 1.0\n",
      "               tarantino = True              neg : pos    =      2.5 : 1.0\n",
      "                  ordell = True              neg : pos    =      2.4 : 1.0\n",
      "                  runner = True              neg : pos    =      2.3 : 1.0\n",
      "                  matrix = True              pos : neg    =      2.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Sklearn classifier with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn and NLTK both requires data to be in different formats: \n",
    "* NLTK classifiers require data in form of array of tuples, where each tuple has dictionary of features and category. \n",
    "* Sklearn classifiers require data in X Y format, X being a 2D array and Y being output. \n",
    "\n",
    "Thus,  NLTK provides a dummy classifier which enables us to use any sklearn classifier with data formed in nltk required format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVC with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "classifier_sklearn = SklearnClassifier(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_sklearn.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.312"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forest with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "classifier_sklearn1 = SklearnClassifier(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_sklearn1.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.312"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn1, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It keeps data in Sklearn required format. \n",
    "\n",
    "* **Tokenisation**: It convert a collection of text documents to a matrix of token counts.\n",
    "* **Feature Extraction** : It picks up the best features.\n",
    "* **Sparse Matrix Creation**: Converts each document into the frequency array, based upon the feautures we have chosen.\n",
    "\n",
    "Important Parameters:\n",
    "1. **max_features** - build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "2. **ngram_range** : tuple (min_n, max_n) The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "3. **analyzer** : string, {‘word’, ‘char’, ‘char_wb’} or callable. Whether the feature should be made of word or character n-grams. \n",
    "4. **max_df** : float in range [0.0, 1.0] or int, default=1.0.\n",
    "    When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold \n",
    "\n",
    "5. **min_df** : float in range [0.0, 1.0] or int, default=1.\n",
    "    When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "\n",
    "\n",
    "Note: \n",
    "1. There is an option in count vectorizer(stop_words) which takes list of stop words and can do the work for us.\n",
    "2. It creates data with frequency counts earlier using nltk we were creating only boolean values for each features.\n",
    "3. Number of N-grams for a sentence with X words will be (X - N + 1) \n",
    "4. Use of N-Grams in your feature space may not necessarily yield any significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dummy training data set \n",
    "training_set = {\"the sky is blue sky\", \"the sun is bright\"}\n",
    "\n",
    "## create a count vectorizer object with max features to be 3\n",
    "count_vec = CountVectorizer(max_features=3)\n",
    "\n",
    "## On fit_transform on training data, Count Vectorizer actually picks the most frequent max_features(in this case 3) \n",
    "## number of words and then convert each training row into count/frequency for these features \n",
    "a = count_vec.fit_transform(training_set)\n",
    "\n",
    "## It returns a sparse matrix i.e. where most of matrix contains 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 2, 1],\n",
       "        [1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'sky', 'the']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Displays the features picked by the countVectorizer\n",
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"he\", \"is\"]\n",
    "\" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Converting our document into x and y\n",
    "categories = [category for document,category in documents]\n",
    "text_document = [\" \".join(document) for document,category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_document, categories, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=2000, ngram_range=(1,2))\n",
    "## Fit transform the training data\n",
    "x_train_features = count_vec.fit_transform(x_train)\n",
    "\n",
    "## Only transform the testing data according to the features which was fit using x_train\n",
    "x_test_features = count_vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying sklearn classifiers\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(x_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32700000000000001"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(x_test_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Term Frequency** : Number of times term t occurs in document d\n",
    "\n",
    "$$ TF(t) = \\frac {\\text{ Number of times term t appears in a document}} {\\text {Total number of terms in the document}} $$\n",
    "** Inverse Document Frequency**:\n",
    "\n",
    "$$ DF(t) = \\frac {\\text{Number   of documents with term t in it}} {\\text{Total Number of Documents}} $$\n",
    "\n",
    "$$IDF(t) = {log_e( \\frac{\\text {Total number of documents}} {\\text{Number of documents with term 't' in it}})}$$\n",
    "\n",
    "\n",
    "Note:\n",
    "1. Higher value of IDF for term t indicates term t is rare in all document collection. \n",
    "2. Sklearn has TF-IDF vectorizer in **sklearn.feature_extraction.text.TfidfVectorizer**.\n",
    "3. We use product (TF * IDF) as data rather than the frequency counts.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
